{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yogi Berra said, 'You can observe a lot by watching'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Chapter 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis\n- The process of organizing, plotting, and summarizing a dataset\n\n\n### John Tukey\nExploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_swing = pd.read_csv('/kaggle/input/swing-states/2008_swing_states.csv')\ndf_swing[['state','county','dem_share']].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n_ = plt.hist(df_swing['dem_share'])\n_ = plt.xlabel('percent of vote for obama')\n_ = plt.ylabel('number of counties')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_edges = [0,10,20,30,40,50,60,70,80,90,100]\n_ = plt.hist(df_swing['dem_share'], bins = bin_edges)\n_ = plt.xlabel('percent of vote for obama')\n_ = plt.ylabel('number of counties')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The height of each bar is the number of counties that had the given level of support for obama","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\n_ = plt.hist(df_swing['dem_share'])\n_ = plt.xlabel('percent of vote for obama')\n_ = plt.ylabel('number of counties')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"square root rule\" is a commonly-used rule of thumb for choosing number of bins: choose the number of bins to be the square root of the number of samples. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import numpy\nimport numpy as np\n\n# Compute number of data points: n_data\nn_data = len(df_swing['dem_share'])\n\n# Number of bins is the square root of number of data points: n_bins\nn_bins = np.sqrt(n_data)\n\n# Convert number of bins to integer: n_bins\nn_bins = int(n_bins)\n\n# Plot the histogram\n_ = plt.hist(df_swing['dem_share'], bins = n_bins)\n\n# Label axes\n_ = plt.xlabel('percent of vote for obama')\n_ = plt.ylabel('number of counties')\n\n# Show histogram\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bee swarm plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_swing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A major drawback of using histograms is the same the dataset can look diferent depending on how the bins are chosen. This lead to binning bias!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Binning bias\n- The same data may be interpreted differently depending on choice of bins\n- Histograms is not plotting all of the data, losing their actual values (sweeping the data into bins)\n\n### Bee swarm plot\n- To remedy histogram plot, you can use bee swarm plot\n- The position along the y-axis is the quantitative information\n- The data are spread in X to make them visible\n- No Binning bias and all data are displayed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x='state',y='dem_share',data=df_swing)\nplt.xlabel('state')\nplt.ylabel('percent of vote for Obama')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obama got less than 50% of the vote in the majority of counties in each of the three swing states","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ECDFs\n- There is a limit to Swarm Plot Efficacy\n- When you have too many data, In the edges data will overlapping. It is obfuscating data\n- Use Empirical cumulative distribution function (ECDF) to overcame this","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.sort(df_swing['dem_share'])\ny = np.arange(1,len(x)+1) / len(x)\n_ = plt.plot(x,y,marker='.', linestyle='none')\n_ = plt.xlabel('percent of vote for obama')\n_ = plt.ylabel('ECDF')\nplt.margins(0.02) # Keeps data off plot edges\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 20% of counties had 36% or less vote for Obama\n- 75% of counties had less that half vote for Obama","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, len(data)+1) / n\n\n    return x, y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\niris_ = pd.DataFrame(X, columns = iris.feature_names)\niris_['Species'] = y\niris_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.target_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"setosa_petal_length = iris_[iris_['Species'] == 0]['petal length (cm)']\nversicolor_petal_length = iris_[iris_['Species'] == 1]['petal length (cm)']\nvirginica_petal_length = iris_[iris_['Species'] == 2]['petal length (cm)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute ECDFs\nx_set,y_set = ecdf(setosa_petal_length)\nx_vers, y_vers = ecdf(versicolor_petal_length)\nx_virg, y_virg = ecdf(virginica_petal_length)\n\n\n# Plot all ECDFs on the same plot\n_ = plt.plot(x_set, y_set, marker='.', linestyle='none')\n_ = plt.plot(x_vers,y_vers, marker='.', linestyle='none')\n_ = plt.plot(x_virg,y_virg, marker='.',linestyle='none')\n\n\n\n# Annotate the plot\nplt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')\n_ = plt.xlabel('petal length (cm)')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chapter 2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Summary Statistics : The sample mean and median","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(df_swing[df_swing['state'] == 'PA']['dem_share'])) # Mean vote for Obama in Pennysylvania\nprint(np.median(df_swing[df_swing['state'] == 'PA']['dem_share'])) # Median vote for Obama in Pennysylvania","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean is not representative when you have extreme values (outliers), but Median is more robust even there is a extreme values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The median is a special name for the 50th percentile. That means 50% of the data are less than the median.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Percentiles on an ECDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(df_swing['dem_share'],[25,50,75])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"quantitaive EDA meets graphical EDA. BOXPLOT !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='state', y= 'dem_share',data=df_swing)\nplt.xlabel('State')\nplt.ylabel('percent of vote for obama')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute ECDFs\nx_set,y_set = ecdf(setosa_petal_length)\nx_vers, y_vers = ecdf(versicolor_petal_length)\nx_virg, y_virg = ecdf(virginica_petal_length)\n\n# Specify array of percentiles: percentiles\npercentiles = np.array([2.5, 25, 50, 75, 97.5])\n\n# Compute percentiles: ptiles_vers\nptiles_vers = np.percentile(versicolor_petal_length, percentiles)\nptiles_set = np.percentile(setosa_petal_length, percentiles)\nptiles_virg = np.percentile(virginica_petal_length, percentiles)\n\n# Plot all ECDFs on the same plot\n_ = plt.plot(x_set, y_set, marker='.', linestyle='none')\n_ = plt.plot(x_vers,y_vers, marker='.', linestyle='none')\n_ = plt.plot(x_virg,y_virg, marker='.',linestyle='none')\n\n# Plot the percentiles point \n_ = plt.plot(ptiles_vers, percentiles/100, marker='D',color='red', linestyle='none')\n_ = plt.plot(ptiles_set, percentiles/100, marker='D',color='red', linestyle='none')\n_ = plt.plot(ptiles_virg, percentiles/100, marker='D',color='red', linestyle='none')\n\n\n# Annotate the plot\nplt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')\n_ = plt.xlabel('petal length (cm)')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.figure(figsize = (15,8));\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a box plot for the petal lengths is unnecessary because the iris data set is not too large and the bee swarm plot works fine. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Species',y='petal length (cm)', data=iris_)\nplt.xlabel('Species')\nplt.ylabel('petal length (cm)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variance and standard deviation\n- The mean squared distance of the data from their mean\n- Informally, a measure of the spread of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.var(df_swing['dem_share']) # It is not have in the same unit of the percentage vote for Obama","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.std(df_swing['dem_share']) # Use the squared root of variance which is called standar deviation to have the same unit of the percentage vote for Obama","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Covariance and the Pearson correlation coefficient","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Scatter plot!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df_swing['total_votes']/1000, df_swing['dem_share'], marker='.',linestyle='none')\nplt.xlabel('total votes (thousands)')\nplt.ylabel('percentage of vote for obama')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want a number that summarizes how Obama's vote share varies with the total vote count","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Covariance\n- A measure of how two quantities vary together\n- We want a number to be dimensionless (not have any units) when we want to see how variables are depend on each other use Pearson correlation coefficient\n\n### Pearson correlation\n$\\rho = \\frac{covariance}{(std\\ of \\ x)(std\\ of \\ y)}$\n\n<t> $\\rho = \\frac{variability \\ due \\ to \\ codependence}{independant \\ variability}$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"covariance_matrix = np.cov(iris_['sepal length (cm)'], iris_['sepal width (cm)'])\nprint(covariance_matrix)\nprint('covariance between length and width :',covariance_matrix[0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pearson_r(x, y):\n    \"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x,y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]\n\n# Compute Pearson correlation coefficient for I. versicolor: r\nr = pearson_r(iris_['sepal length (cm)'],iris_['sepal width (cm)'])\n\n# Print the result\nprint(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_.corr() # In pandas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chapter 3","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Probabilistic logic and statistical inference","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Example repeats of 50 measurements of petal length !\n- Given a set of data, you describe probabilitically what you might expect if those data were acquired again and again and again. This is the heart of statistical inference.\n- Statistical inference is the process by which we go from measured data to probabilistic conclusions about what we might expect if we collected the same data again.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Random number generators and hacker statistics\n- Uses simulated repeated measurements to compute probabilites.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The np.random module\n- Suite of functions based on random number generation\n- np.random.random(): draw a number between 0 and 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.random()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bernoulli trial \n- An experiment that has two options, 'success'(True) and 'failure'(False).\n\n### Random number seed\n- Integer fed into random number generating algorithm\n- Manually seed random number generator if you need reproducibility\n- Specified using np.random.seed()","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Simulating 4 coin flips","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nrandom_numbers = np.random.random(size=4)\nrandom_numbers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = random_numbers < 0.5\nheads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(heads)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find probabilities for 4 heads","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_all_heads = 0 # Initialize number of 4-heads trials\nfor i in range(10000):\n    heads = np.random.random(size=4) < 0.5\n    n_heads = np.sum(heads)\n    if n_heads == 4:\n        n_all_heads += 1\n        \nprint('Probabilities for 4 heads is', n_all_heads/10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hacker stats probabilities\n- Determine how to simulate data\n- Simulate many many times\n- Probability is approximately fraction of trials with the outcome of interest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed the random number generator\nnp.random.seed(42)\n\n# Initialize random numbers: random_numbers\nrandom_numbers = np.empty(100000)\n\n# Generate random numbers by looping over range(100000)\nfor i in range(100000):\n    random_numbers[i] = np.random.random()\n\n# Plot a histogram\n_ = plt.hist(random_numbers)\n\n# Show the plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_bernoulli_trials(n, p):\n    \"\"\"Perform n Bernoulli trials with success probability p\n    and return number of successes.\"\"\"\n    # Initialize number of successes: n_success\n    n_success = 0\n\n    # Perform trials\n    for i in range(n):\n        # Choose random number between zero and one: random_number\n        random_number = np.random.random()\n\n        # If less than p, it's a success so add one to n_success\n        if random_number < p:\n            n_success += 1\n\n    return n_success","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem Example:\n### How many defaults might we expect?\n\nLet's say a bank made 100 mortgage loans. It is possible that anywhere between 0 and 100 of the loans will be defaulted upon. You would like to know the probability of getting a given number of defaults, given that the probability of a default is p = 0.05. To investigate this, you will do a simulation. Perform 100 Bernoulli trials using the perform_bernoulli_trials() function and record how many defaults we get. Here, a success is a default. (Remember that the word \"success\" just means that the Bernoulli trial evaluates to True, i.e., did the loan recipient default?) \n\n<t> If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed random number generator\nnp.random.seed(42)\n\n# Initialize the number of defaults: n_defaults\nn_defaults = np.empty(1000)\n\n# Compute the number of defaults\nfor i in range(1000):\n    n_defaults[i] = perform_bernoulli_trials(100, 0.05)\n\n# Plot the histogram with default number of bins; label your axes\n_ = plt.hist(n_defaults)\n_ = plt.xlabel('number of defaults out of 100 loans')\n_ = plt.ylabel('probability')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute ECDF: x, y\nx,y = ecdf(n_defaults)\n\n# Plot the ECDF with labeled axes\n_ = plt.plot(x,y,marker='.', linestyle='none')\n_ = plt.xlabel('n_defaults')\n_ = plt.ylabel('ECDF')\n\n\n# Show the plot\nplt.show()\n\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\nn_lose_money = np.sum(n_defaults >= 10)\n\n# Compute and print probability of losing money\nprint('Probability of losing money =', n_lose_money / len(n_defaults))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Binomial distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Probability mass function (PMF)\n- The set of probabilities of discrete outcomes\n\n### Discrete Uniform PMF\n- has the same probabilities, for instance: Dice\n\n### Probability distribution\n- A mathematical description of outcomes\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The outcome of rolling a single dice is:\n- Discrete\n- Uniformly distributed\n\n<t> 4 flips coins story is called binomial distribution\n- The number r of successes in n bernoulli trials with probability p of success, is binomially distributed\n- The number r of heads in 4 coin flips with probability 0.5 of heads, is binomially distributed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Sampling from the Binomial distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.binomial(4,0.5) # 4 is the n trials, and 0.5 is the probability of heads","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get 2 heads out of four","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.binomial(4,0.5,size=10) # Size is how many experiments we want to repeat the four flip experiment over and over again. In this case 10 repeats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The binomial PMF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The binomial CDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nx,y = ecdf(np.random.binomial(60,0.1,size=10000))\nplt.plot(x,y,marker='.',linestyle='none')\nplt.xlabel('number of successes')\nplt.ylabel('CDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the binomial PMF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute bin edges: bins\nbins = np.arange(0, max(n_defaults) + 1.5) - 0.5\n\n# Generate histogram\n_ = plt.hist(n_defaults, bins=bins) # normed = True didn't work\n\n# Label axes\n_ = plt.xlabel('number of defaults out of 100 loans')\n_ = plt.ylabel('PMF')\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Poisson processes and the Poisson distribution\n- The timing of the next event is completely indpendent of when the previous event happened","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Examples of Poisson processes\n- Natural births in a given hospital\n- Hit on a website during a given hour\n- Meteor strikes\n- Molecular collisions in a gas\n- Aviation incidents\n- Buses in Poissonville","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The number of arrivals of a Poisson process in a given amount of time is Poisson distributed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Poisson distribution\n- The poisson distribution has one parameter, the average number of arrivals in a given length of time\n- The number r of hits on a website in one hour with an average hit rate of 6 hits per hour is Poisson distributed\n- Limit of the Binomial distribution for low probability of success and large number of trials\n- That is, for rare events","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###  The Poisson CDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"samples = np.random.poisson(6, size=10000)\nx,y = ecdf(samples)\nplt.plot(x,y,marker='.', linestyle='none')\nplt.margins(0.2)\nplt.xlabel('number of successes')\nplt.ylabel('CDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw 10,000 samples out of Poisson distribution: samples_poisson\nsamples_poisson = np.random.poisson(10, size=10000)\n\n# Print the mean and standard deviation\nprint('Poisson:     ', np.mean(samples_poisson),\n                       np.std(samples_poisson))\n\n# Specify values of n and p to consider for Binomial: n, p\nn = [20,100,1000]\np = [0.5,0.1,0.01]\n\n\n# Draw 10,000 samples for each n,p pair: samples_binomial\nfor i in range(3):\n    samples_binomial = np.random.binomial(n[i],p[i],size=10000)\n\n    # Print results\n    print('n =', n[i], 'Binom:', np.mean(samples_binomial),\n                                 np.std(samples_binomial))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Problem\nIn baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. \n### Was 2015 anomalous?\n1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average 251/115 no-hitters per season, what is the probability of having seven or more in a season?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw 10,000 samples out of Poisson distribution: n_nohitters\nn_nohitters = np.random.poisson(251/115, size=10000)\n\n# Compute number of samples that are seven or greater: n_large\nn_large = np.sum(n_nohitters >= 7)\n\n# Compute probability of getting seven or more: p_large\np_large = n_large/10000\n\n# Print the result\nprint('Probability of seven or more no-hitters:', p_large)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chapter 4","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Probability density function","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Continous variables\nQuantities that can take any value, not just discrete values\n\n### Michelson's speed of light experiment\nMeasured speed of light (1000km/s) for 100 experiments, What probability distribution describes these data?\n<t> These data follow normal distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Probability density function\n- Continuous analog to the PMF\n- Mathematical description of the relative likelihood of observing a value of a continuous variable","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Areas under the PDF give probabilities","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Introduction to the Normal Distribution\n- Describes a continuous variable whose PDF has a single symmetric peak\n- The normal distribution is parameterized by two variables:\n    - The mean determine where the center of peak is\n    - The standard deviation is a measure of how wide the peak is or how spread out the data are","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking normality of a dataset\n```python\nmean = np.mean(data)\nstd = np.std(data)\n\nsamples = np.random.normal(mean,std,size=10000)\nx,y = ecdf(data)\nx_theory, y_theory = ecdf(samples)\n\nplt.plot(x_theory, y_theory)\nplt.plot(x,y, marker='.', linestyle='none')\nplt.xlabel('speed of light (km/s)')\nplt.ylabel('CDF')\nplt.show()```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Draw 100000 samples from Normal distribution with stds of interest: samples_std1, samples_std3, samples_std10\nsamples_std1 = np.random.normal(20,1, size=100000)\nsamples_std3 = np.random.normal(20,3,size=100000)\nsamples_std10 = np.random.normal(20,10,size=100000)\n\n# # Make histograms\n# _ = plt.hist(samples_std1,normed=True,histtype='step', bins=100)\n# _ = plt.hist(samples_std3,normed=True,histtype='step',bins=100)\n# _ = plt.hist(samples_std10,normed=True,histtype='step',bins=100)\n\n# # Make a legend, set limits and show plot\n# _ = plt.legend(('std = 1', 'std = 3', 'std = 10'))\n# plt.ylim(-0.01, 0.42)\n# plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate CDFs\nx_std1, y_std1 = ecdf(samples_std1)\nx_std3, y_std3 = ecdf(samples_std3)\nx_std10, y_std10 = ecdf(samples_std10)\n\n\n# Plot CDFs\n_ = plt.plot(x_std1, y_std1, marker='.',linestyle='none')\n_ = plt.plot(x_std3, y_std3, marker='.',linestyle='none')\n_ = plt.plot(x_std10, y_std10, marker='.',linestyle='none')\n\n\n# Make a legend and show the plot\n_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The normal distribution: Properties and warnings\n- It is called gaussian distribution\n    - It is used to describe most symmetric peaked data you will encounter\n    - Normality about the data are present\n    - This is very powerful distribution that seems to be ubiquitous in nature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The exponential distribution\n- The waiting time between arrivals of a Poisson process is Exponentially distributed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The exponential and normal are just two of many examples of continuous probability distributions\n<t> the Exponential distribution describes the waiting times between rare events, and Secretariat is rare!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### If you have a story, you can simulate it!\n\nSometimes, the story describing our probability distribution does not have a named distribution to go along with it. In these cases, fear not! You can always simulate it. We'll do that in this and the next exercise.\n\n<t> In earlier exercises, we looked at the rare event of no-hitters in Major League Baseball. Hitting the cycle is another rare baseball event. When a batter hits the cycle, he gets all four kinds of hits, a single, double, triple, and home run, in a single game. Like no-hitters, this can be modeled as a Poisson process, so the time between hits of the cycle are also Exponentially distributed.\n\n<t> How long must we wait to see both a no-hitter and then a batter hit the cycle? The idea is that we have to wait some time for the no-hitter, and then after the no-hitter, we have to wait for hitting the cycle. Stated another way, what is the total waiting time for the arrival of two different Poisson processes? The total waiting time is the time waited for the no-hitter, plus the time waited for the hitting the cycle.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Define a function with call signature successive_poisson(tau1, tau2, size=1) that samples the waiting time for a no-hitter and a hit of the cycle.\n- Draw waiting times tau1 (size number of samples) for the no-hitter out of an exponential distribution and assign to t1.\n- Draw waiting times tau2 (size number of samples) for hitting the cycle out of an exponential distribution and assign to t2.\n- The function returns the sum of the waiting times for the two events.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def successive_poisson(tau1, tau2, size=1):\n    \"\"\"Compute time for arrival of 2 successive Poisson processes.\"\"\"\n    # Draw samples out of first exponential distribution: t1\n    t1 = np.random.exponential(tau1, size)\n\n    # Draw samples out of second exponential distribution: t2\n    t2 = np.random.exponential(tau2, size)\n\n    return t1 + t2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, you'll use your sampling function to compute the waiting time to observe a no-hitter and hitting of the cycle. The mean waiting time for a no-hitter is 764 games, and the mean waiting time for hitting the cycle is 715 games.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw samples of waiting times: waiting_times\nwaiting_times = successive_poisson(764,715, size=100000)\n\n# Make the histogram\n_ = plt.hist(waiting_times, bins=100,histtype='step')\n\n\n# Label axes\n_ = plt.xlabel('Total times')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Construct (beatiful) instructive plots\n- Compute informative summary statistics\n- Use hacker statistics\n- Think probabilistically","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Chapter 6","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Outcomes of measurements follow probability distributions defined by the story of how the data came to be\n\n<t> PDF and CDF is effective because there is no binning bias","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Optimal Parameters\n- How did we know that mean and standard deviation calculated from the data were the appropriate values for the normal parameters?\n- We could have chose anothers, and the ECDF result will be differs\n\n<t> \nIf we believe that the process that generates our data gives normally distributed results, the set of parameters that brings the model, These are the optimal parameters\n \n- Parameter values that bring the model in closest agreement with the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note: The parameters are only optimal for the model you chose for your data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Packages to do statistical inference\n1. scipy.stats\n2. statsmodels\n3. hacker stats with numpy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The number of games played between each no-hitter in the modern era (1901-2015) of Major League Baseball is stored in the array nohitter_times. \n\n<t> If you assume that no-hitters are described as a Poisson process, then the time between no-hitters is Exponentially distributed. As you have seen, the Exponential distribution has a single parameter, which we will call τ, the typical interval time. The value of the parameter τ that makes the exponential distribution best match the data is the mean interval time (where time is in units of number of games) between no-hitters. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nohitter_times = [843,1613,1101,215,684,814,278, 20,123,299,324,161,219,545,124,1468,983,715,966,624,29,450,62,1878,1104,107,91,1325,104,1309,429,251,\n                  93,39,188,66,702,96,308,1114,23,524,26,645,2088,59,12,2,886,1665,548,1525,813,887,42,2090,750,4021,1070,1765,1322,11,1084,2900,2432,\n                  77,2181,2752,557,2267,1023,1194,219,1531,26,127,2147,211,41,1575,233,996,600,151,479,697,542,462,603,392,583,73,397,1529,528,480,\n                  1497,52,255,37,943,717,224,44,498,216,288,267,269,1086,386,176,2199,509,675,1243,463,12,1124,650,171,327,110,54,197,774,8,136,380,\n                  811,232, 64,323,240,986,179,203,192,539,1491,702,715,82,1397,445,731,226,605,156,354,778,603,1001,385,149,675,1351,2983,1568,79,\n                  242,180,1403,45,899,252,100,2055,4043,206,55,576,595,238,3931,2351,179,3260,1025,31,660,577,157,192, 0,1693,110, 215, 563, 1848, \n                  388, 225,1134,1172,1555,31,192,792,693,55,467, 239, 119, 905, 876, 381, 2491, 1202, 1056, 1549, 1520, 950]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed random number generator\nnp.random.seed(42)\n\n# Compute mean no-hitter time: tau\ntau = np.mean(nohitter_times)\n\n# Draw out of an exponential distribution with parameter tau: inter_nohitter_time\ninter_nohitter_time = np.random.exponential(tau, 100000)\n\n# Plot the PDF and label axes\n_ = plt.hist(inter_nohitter_time,\n             bins=50, histtype='step')\n_ = plt.xlabel('Games between no-hitters')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an ECDF from real data: x, y\nx, y = ecdf(nohitter_times)\n\n# Create a CDF from theoretical samples: x_theor, y_theor\nx_theor, y_theor = ecdf(inter_nohitter_time)\n\n# Overlay the plots\nplt.plot(x_theor, y_theor)\nplt.plot(x, y, marker='.', linestyle='none')\n\n# Margins and axis labels\nplt.margins(0.02)\nplt.xlabel('Games between no-hitters')\nplt.ylabel('CDF')\n\n# Show the plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the theoretical CDFs\nplt.plot(x_theor, y_theor)\nplt.plot(x, y, marker='.', linestyle='none')\nplt.margins(0.02)\nplt.xlabel('Games between no-hitters')\nplt.ylabel('CDF')\n\n# Take samples with half tau: samples_half\nsamples_half = np.random.exponential(tau/2,size=10000)\n\n# Take samples with double tau: samples_double\nsamples_double = np.random.exponential(2*tau,size=10000)\n\n# Generate CDFs from these samples\nx_half, y_half = ecdf(samples_half)\nx_double, y_double = ecdf(samples_double)\n\n# Plot these CDFs as lines\n_ = plt.plot(x_half, y_half)\n_ = plt.plot(x_double, y_double)\n\n# Show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how the value of tau given by the mean matches the data best. In this way, tau is an optimal parameter.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression by least squares\nThe slop sets how steep the line is, and the intercept sets where the line crosses the y-axis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Least squares\n- The process of finding the parameters for which the sum of the squares of the residuals is minimal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_swing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"slope, intercept = np.polyfit(df_swing['total_votes'], df_swing['dem_share'], 1) # 1 indicates the degree of polynomial. 1 is linear regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"slope,intercept","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.plot(df_swing['total_votes'], df_swing['dem_share'], marker='.', linestyle='none')\nplt.margins(0.02)\n_ = plt.xlabel('total_votes')\n_ = plt.ylabel('percent vote for Obama')\n                                               \n# Make theoretical line to plot\nx = np.array([0, 1000000])\ny = slope * x + intercept\n\n# Add regression line to your plot\n_ = plt.plot(x, y)\n\n# Draw the plot\nplt.show()\n\n                                               ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The slope tells us that we get abot 4 more percent votes for obama for every 100,000 additional voters in a country","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify slopes to consider: a_vals\na_vals = np.linspace(0, 0.1, 200)\n\n# Initialize sum of square of residuals: rss\nrss = np.empty_like(a_vals)\n\n# Compute sum of square of residuals for each value of a_vals\nfor i, a in enumerate(a_vals):\n    rss[i] = np.sum((df_swing['dem_share'] - a*df_swing['total_votes'] - intercept)**2)\n\n# Plot the RSS\nplt.plot(a_vals, rss, '-')\nplt.xlabel('total_votes')\nplt.ylabel('percent vote for Obama')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It doesn't have convex curve ! So it isn't optimal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The importance of EDA: Anscombe's quartet\n- Don't do blindly parameter estimation\n- There is 4 datasets but this 4 datasets have the same mean,median, and even linear regression ! \n- But if you do the EDA, you can see how the point is distributed ! \n- Linearly, Exponentially, and Randomly ! \n- Do graphical EDA first ! and judge your data is it linear or exponential ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Chapter 7","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Generating bootstrap replicates\n### Bootstrapping\n- The use of resampled data to perform statistical inference\n- A resampled array of the data --> Bootstrap sample\n- A statistic computed from a resampled array --> Bootstrap replicate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.choice([1,2,3,4,5], size=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bootstrap confidence Intervals","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bootstrap replicate function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bootstrap_replicate_1d(data,func):\n    \"\"\"Generate bootstrap replicate of 1D data\"\"\"\n    bs_sample = np.random.choice(data,len(data))\n    return func(bs_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bootstrap_replicate_1d(df_swing['dem_share'], np.mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bootstrap_replicate_1d(df_swing['dem_share'], np.mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Many bootstrap replicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bs_replicates = np.empty(10000)\nfor i in range(10000):\n    bs_replicates[i] = bootstrap_replicate_1d(df_swing['dem_share'], np.mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(bs_replicates, bins=30)\nplt.xlabel('mean of percentage vote for Obama')\nplt.ylabel('PDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bootstraping estimate of the mean\n- It is useful to summarize result without having to resort to a graphical method like a histogram\n\n### Confidence interval of a statistic\n- If we repeated measurements over and over again, p% of the oberserved values would lie within the p% confidence interval","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_int = np.percentile(bs_replicates, [2.5,97.5])\nconf_int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_bs_reps(data, func, size=1):\n    \"\"\"Draw bootstrap replicates.\"\"\"\n\n    # Initialize array of replicates: bs_replicates\n    bs_replicates = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_replicates[i] = bootstrap_replicate_1d(data,func)\n\n    return bs_replicates\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) The standard deviation of this distribution, called the standard error of the mean, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, sem = np.std(data) / np.sqrt(len(data)). Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Consider again the inter-no-hitter intervals for the modern era of baseball. Generate 10,000 bootstrap replicates of the optimal parameter \bτ. Plot a histogram of your replicates and report a 95% confidence interval.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates\nbs_replicates = draw_bs_reps(nohitter_times, np.mean,size=10000)\n\n# Compute the 95% confidence interval: conf_int\nconf_int = np.percentile(bs_replicates, [2.5,97.5])\n\n# Print the confidence interval\nprint('95% confidence interval =', conf_int, 'games')\n\n# Plot the histogram of the replicates\n_ = plt.hist(bs_replicates, bins=50)\n_ = plt.xlabel(r'$\\tau$ (games)')\n_ = plt.ylabel('PDF')\n\n# Show the plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 660 and 870 games.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pairs bootstrap\n- When we computed bootstrap confidence intervals on summary statistics,we did nonparametrically\n\n### Nonparametric inference\n- Make no assumptions about the model or probability distribution underlying the data, the estimates were done using the data alone\n- Linear regression is parametric estimate","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2008 US swing state election results\nWhat if we had the election again,under identical conditions? How would the slope and intercept change?\n- There are several ways to get bootstrap estimates of the confidence intervals on these parameters, each of which makes difference assumptions about the data\n- Need a method to least assumptions it is called pairs bootstrap","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pairs bootstrap for linear regression\n- Resample data in pairs\n- Compute slope and intercept from resapmled data\n- Each slope and intercept is a bootstrap replicate\n- Compute confidence intervals from percentiles of bootstrap replicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inds = np.arange(len(df_swing['total_votes']))\nbs_inds = np.random.choice(inds,len(inds))\nbs_total_votes = df_swing['total_votes'][bs_inds]\nbs_dem_share = df_swing['dem_share'][bs_inds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs_slope, bs_intercept = np.polyfit(bs_total_votes, bs_dem_share, 1)\nbs_slope,bs_intercept","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can plot the lines you get from the bootstrap replicate to get graphical idea how the regression line may change if the data were collected again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_bs_pairs_linreg(x, y, size=1):\n    \"\"\"Perform pairs bootstrap for linear regression.\"\"\"\n\n    # Set up array of indices to sample from: inds\n    inds = np.arange(len(x))\n\n    # Initialize replicates: bs_slope_reps, bs_intercept_reps\n    bs_slope_reps = np.empty(size)\n    bs_intercept_reps = np.empty(size)\n\n    # Generate replicates\n    for i in range(size):\n        bs_inds = np.random.choice(inds, size=len(inds))\n        bs_x, bs_y = x[bs_inds], y[bs_inds]\n        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)\n\n    return bs_slope_reps, bs_intercept_reps\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate replicates of slope and intercept using pairs bootstrap\nbs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(df_swing['total_votes'],df_swing['dem_share'], size= 1000)\n\n# Compute and print 95% CI for slope\nprint(np.percentile(bs_slope_reps, [2.5,97.5]))\n\n# Plot the histogram\n_ = plt.hist(bs_slope_reps, bins=50)\n_ = plt.xlabel('slope')\n_ = plt.ylabel('PDF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate array of x-values for bootstrap lines: x\nx = np.array([0, 1000000])\n\n# Plot the bootstrap lines\nfor i in range(100):\n    _ = plt.plot(x, \n                 bs_slope_reps[i] * x + bs_intercept_reps[i],\n                 linewidth=0.5, alpha=0.2, color='red')\n\n# Plot the data\n_ = plt.plot(df_swing['total_votes'], df_swing['dem_share'], marker='.', linestyle='none')\n\n# Label axes, set the margins, and show the plot\n_ = plt.xlabel('total_votes')\n_ = plt.ylabel('percentage vote for obama')\nplt.margins(0.02)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chapter 8\nHow to assess how reasonable it is that our observed data are actually described by the model?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Hypothesis testing\n- Assesment of how reasonable the observed data are assuming a hypothesis is true\n\n### Null hypothesis\n- Another name for the hypothesis you are testing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Simulating the hypothesis\n- Simulate what the data would look like if the county-level voting trends in the two states were identically distributed ?\n\n### Permutation\n- Random reordering of entries in an array\n- This is the heart of simulating a null hypothesis were we assume two quantities are identically distributed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Generating a permutation sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dem_share_PA = df_swing[df_swing['state'] == 'PA']['dem_share']\ndem_share_OH = df_swing[df_swing['state'] == 'OH']['dem_share']\n\ndem_share_both = np.concatenate((dem_share_PA, dem_share_OH))\ndem_share_perm = np.random.permutation(dem_share_both)\nperm_sample_PA = dem_share_perm[:len(dem_share_PA)]\nperm_sample_OH = dem_share_perm[len(dem_share_PA):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1,data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test statistics and p-values\nHow do we quantify the assesment ?\n\n### Test statistic\n- A single number that can be computed from observed data and from data you simulate under the null hypothesis\n- It serves as a basis of comparison between what the hypothesis predicts and what we actually observed\n- Choose your test statistic to be something pertinent to the question you are trying to answer with your hypothesis test, in this case are the two states different ?\n- If they are identical, they should have the same mean vote share for Obama. So the difference in mean vote share should be zero","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Permutation replicate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(perm_sample_PA) - np.mean(perm_sample_OH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(dem_share_PA) - np.mean(dem_share_OH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### p-value\n- The probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true\n- NOT the probability that the null hypothesis is true\n\n### Statistical significance\n- Determine by the smallness of a p-value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"p-value is a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1)-np.mean(data_2)\n\n    return diff","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compute p-value\n```python\ndef diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1) - np.mean(data_2)\n\n    return diff\n\n# Compute difference of mean impact force from experiment: empirical_diff_means\nempirical_diff_means = diff_of_means(force_a, force_b)\n\n# Draw 10,000 permutation replicates: perm_replicates\nperm_replicates = draw_perm_reps(force_a, force_b,\n                                 diff_of_means, size=10000)\n\n# Compute p-value: p\np = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)\n\n# Print the result\nprint('p-value =', p)```\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pipeline for hypothesis testing\n- Clearly state the null hypothesis\n- Define your test statistic\n- Generate many sets of simulated data assuming the null hypothesis is true\n- Compute the test statistic for each simulated data test\n- The p-value is the fraction of your simulated data sets for which the test statistics is at least as extreme as for the real data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### One sample test\n- Compare one set of data to a single number\n\n### Two sample test\n- Compare two sets of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}